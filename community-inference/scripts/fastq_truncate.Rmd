---
title: "FASTQ Filtering Script"
author: "Vincent Caruso"
date: "July 9, 2017"
output: html_document
---

```{r setup, include=FALSE}

library("knitr")
knitr::opts_chunk$set(echo = TRUE)
#opts_knit$set(root.dir = "~/projects/thesis/data")
```


##Get working directory from command line

```{r}

library(optparse)
 
option_list = list(
  make_option(c("-d", "--data"), type = "character", default = NULL, 
              help = "data directory"),
  make_option(c("-f", "--ftrunc"), type = "integer", default = 230,
              help = "forward read truncate position"),
  make_option(c("-b", "--rtrunc"), type = "integer", default = 210,
              help = "reverse read truncate position")
)
 
opt_parser = OptionParser(option_list=option_list)
opt = parse_args(opt_parser)

```


Set up the environment.
```{r}
#source("https://bioconductor.org/biocLite.R")
#biocLite("dada2")
library("dada2"); packageVersion("dada2")
library("ggplot2")
library("stringr")

```


##Filtering strategy

There are two strategies I'm considering for quality filtering the raw .fastq files from the mock community dilution series sequencing experiment:

1. Merge forward and reverse reads first, taking advantage of potentially improved posterior Q scores that result from overlapping reads, and then quality filter the merged reads based on expected errors. This approach entails the use of the `fastq_mergepairs` function in USEARCH.

2. Truncate the low-quality heads and tails of forward and reverse reads independently, then merge the truncated reads, and finally quality filter the merged reads, again based on expected errors. This approach requires the use of the `fastqPairedFilter` function in DADA2 for the initial truncation step, since only this filter maintains the consistent ordering of the forward and reverse reads.

Here, I'll use approach #2. In this script, I'm only performing truncation (as well as removing any residual phiX) with `fastqPairedFilter`, since I still want to merge the resulting .fastq files with USEARCH's `fastq_mergepairs` before doing quality filtering. I'll also use USEARCH to do quality filtering with `fastq_filter`. 

##Rename files

First things first: the file names are really long and contain redundant information, so I'll rename them to remove the unnecessary bits.
```{r rename files}

data_path <- "~/thesis/data/zymo_neat"
data_path <- opt$data
raw_path <- file.path(data_path, "raw")

file_names <- list.files(raw_path)
fastqs <- str_subset(file_names, ".fastq$")

# rename files in MockCommunities folder

if (str_detect(fastqs[1], "^lane1-")){
  for (i in seq_along(fastqs)){
    new_name <- fastqs[i]
    new_name <- str_replace(new_name, "lane1-", "")
    new_name <- str_replace(new_name, "index-[ACGT]+-", "")
    new_name <- str_replace(new_name, "_S\\d{3}_L001", "")
    new_name <- str_replace(new_name, "_001", "")
    #new_name <- str_replace(new_name, "-", ".")
    file.rename(file.path(raw_path, fastqs[i]), file.path(raw_path, new_name))
  }
}


```

##Plot quality profile

First, I'll look at the quality profile of each read set to determine where to truncate.
```{r}

fastq_Fs <- str_subset(fastqs, "_R1")
fastq_Rs <- str_subset(fastqs, "_R2")

#get the sample names
sample_names <- sapply(str_split(fastq_Fs, "_R\\d"), `[`, 1)
# replace '-' with '.' in sample names
#sample_names <- str_replace_all(sample_names, "-", "\\.")

qual_path <- file.path(raw_path, "quality")
if (!file_test("-d", qual_path)) dir.create(qual_path)


#plot quality
if (length(list.files(qual_path)) == 0){
  for (fq in fastqs){
    plotQualityProfile(file.path(raw_path, fq)) +
    scale_y_continuous(limits = c(10, 40), breaks = seq(10, 40, 5)) +
    scale_x_continuous(limits = c(0, 250), breaks = seq(0, 250, 10)) +
    theme(panel.grid.major = element_line(colour="grey", size=0.5)) +
    ggtitle(str_replace(fq, ".fastq", ""))
  
    fname <- str_replace(fq, "fastq", "png")
    ggsave(file.path(qual_path, fname), width = 10, height = 7)
  }
}

```


##Truncate .fastq's

Based on the quality profiles, I'll truncate the forward reads at position 230, and the reverse at position 210. I'll also strip the first 10 nucleotides.
```{r}

# Set up a directory and file names for the truncated reads
trunc_path <- file.path(data_path, "truncated")
if (!file_test("-d", trunc_path)) dir.create(trunc_path)

trunc_Fs <- paste0(sample_names, "_trunc_R1.fastq")
trunc_Rs <- paste0(sample_names, "_trunc_R2.fastq")

# Truncate and filter paired read sets
for (i in seq_along(fastq_Fs)){
  fastqPairedFilter(file.path(raw_path, c(fastq_Fs[i], fastq_Rs[i])), file.path(trunc_path, c(trunc_Fs[i], trunc_Rs[i])),
                    truncLen = c(opt$ftrunc, opt$rtrunc), trimLeft = c(15, 15),
                    #truncLen = c(240, 220), trimLeft = c(15, 15),
                    maxN = Inf, maxEE = c(Inf, Inf), truncQ = 2, rm.phix = TRUE,
                    compress = FALSE, verbose = TRUE)
}

```

