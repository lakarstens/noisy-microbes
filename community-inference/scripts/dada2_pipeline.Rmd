---
title: "dada2_pipeline"
author: "Vincent Caruso"
date: "July 20, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Setup

First, load required libraries.
```{r libraries}

library("dada2")
library("stringr")
library("ggplot2")

```


##Set up working directories

Get working directory from command line
```{r}

library(optparse)
 
option_list = list(
  make_option(c("-i", "--input"), type = "character", default = NULL, 
              help = "input directory", metavar = NULL),
  make_option(c("-o", "--output"), type = "character", default = NULL,
              help = "output directory", metavar = NULL),
  # make_option(c("-f", "--ftrunc"), type = "integer", default = 230,
  #             help = "forward read truncate position"),
  # make_option(c("-b", "--rtrunc"), type = "integer", default = 210,
  #             help = "reverse read truncate position"),
  make_option(c("-s", "--min_len"), type = "integer", default = 221, 
              help = "minimum merged length"),
  make_option(c("-l", "--max_len"), type = "integer", default = 225,
              help = "maximum merged length"),
  make_option(c("-F", "--maxee_F"), type = "numeric", default = 2.5,
              help = "max EE forward reads"),
  make_option(c("-R", "--maxee_R"), type = "numeric", default = 2.5,
              help = "max EE reverse reads"),
  make_option(c("-p", "--pooled"), action = "store_true", type = "logical", 
              default = FALSE, help = "pool samples for inference")
)
 
opt_parser = OptionParser(option_list=option_list)
opt = parse_args(opt_parser)

cat("DADA2 parameters:\n")
cat("Input directory:", opt$input, "\n")
cat("Output directory:", opt$output, "\n")
cat("Minimum sequence length:", opt$min_len, "\n")
cat("Maximum sequence length:", opt$max_len, "\n")
cat("Max EE forward reads:", opt$maxee_F, "\n")
cat("Max EE reverse reads:", opt$maxee_R, "\n")

```

Next, define the working directory and file paths.
```{r paths}

data_path <- opt$input     # parent directory for raw and filtered data
dada2_path <- opt$output    # directory for outputs of DADA2 read processsing
# data_path <- "~/thesis/data/test"
# dada2_path <- file.path(data_path, "dada2")     # directory where DADA2 processing results will be stored
trunc_path <- file.path(data_path, "truncated")
filt_path <- file.path(dada2_path, "filtered")     # directory where filtered reads will be stored
# ref_path <- "~/thesis/references"    # directory containing reference databases
# raw_path <- file.path(data_path, "raw")     # directory containing raw read files

if (!file_test("-d", dada2_path)) dir.create(dada2_path)
if (!file_test("-d", filt_path)) dir.create(filt_path)

```


##Filtering and Trimming

Get raw data file names, split them into forward and reverse read files, and infer sample names from the file names.
```{r plot qualities}

#setwd(raw_path)
file_names <- list.files(trunc_path)
fastqs <- str_subset(file_names, ".fastq$")
trunc_Fs <- str_subset(fastqs, "_R1")
trunc_Rs <- str_subset(fastqs, "_R2")

#get the sample names
sample_names <- sapply(str_split(trunc_Fs, "_trunc_R\\d"), `[`, 1)

```

The quality isn't great for most of the read sets. For the "Neat" sample, I'll trim the forward reads at position 230 for now, and the reverse reads at position 210. I'm also trimming the first 15 nucleotides-- the Illumina "burn-in".
```{r filter}

# Define file names for the filtered reads
filt_Fs <- paste0(sample_names, "_filt_R1.fastq")
filt_Rs <- paste0(sample_names, "_filt_R2.fastq")

# Filter paired read sets
filt_stats <- filterAndTrim(fwd = file.path(trunc_path, trunc_Fs), filt = file.path(filt_path, filt_Fs),
                            rev = file.path(trunc_path, trunc_Rs), filt.rev = file.path(filt_path, filt_Rs),
                            #truncLen = c(240, 220), trimLeft = 15, maxEE = c(3, 4), truncQ = 2, rm.phix = TRUE, 
                            maxEE = c(opt$maxee_F, opt$maxee_R), 
                            #truncLen = c(opt$ftrunc, opt$rtrunc), trimLeft = 15, truncQ = 2, rm.phix = TRUE, 
                            compress = FALSE, verbose = TRUE, multithread = TRUE)

```


##Error parameter estimation

Learn the error rates from the data.
```{r errors}

err_F <- learnErrors(file.path(filt_path, filt_Fs), multithread = TRUE)
err_R <- learnErrors(file.path(filt_path, filt_Rs), multithread = TRUE)

```


##Dereplication

Collapse sequence replicates into single sequences, each with a summary of the quality scores at each base position.
```{r dereplicate}

derep_Fs <- derepFastq(file.path(filt_path, filt_Fs), verbose = TRUE)
derep_Rs <- derepFastq(file.path(filt_path, filt_Rs), verbose = TRUE)

#names(derep_Fs) <- sample_names
#names(derep_Rs) <- sample_names

```


##Inference of sequence variants

Since I used all the read data to learn the error rates, the sequence inference for all samples has already been done by DADA2. However, I'll do the inference again formally here, for consistency, using the learned error rates.
```{r SV inference}

dada_Fs <- dada(derep_Fs, err = err_F, multithread = TRUE, pool = opt$pooled)
dada_Rs <- dada(derep_Rs, err = err_R, multithread = TRUE, pool = opt$pooled)

# Save the dada objects
save(err_F, err_R, derep_Fs, derep_Rs, dada_Fs, dada_Rs, file = file.path(dada2_path, "dada2.RData"))

```


##Merging of paired reads

Like it says, now I'm going to merge the paired reads. This will reduce the number of spurious sequences.
```{r merge SVs}

#load(file = file.path(dada2_path, "dada2.RData"))
mergers <- mergePairs(dada_Fs, derep_Fs, dada_Rs, derep_Rs, 
                     propagateCol = c("n0", "n1", "birth_fold", "birth_ham"), 
                     verbose = TRUE)

```


##Create a sequence table

This converts the inferred sequence data into a table, similar to an OTU table.
```{r sequence table}

sv_table <- makeSequenceTable(mergers)
row.names(sv_table) <- sample_names

print("Sequence lengths before length filtering:")
table(nchar(getSequences(sv_table)))

```

If there are any sequences with lengths outside the expected range for the V4 region, I want to remove those.
```{r remove bad lengths}

min_len <- opt$min_len
max_len <- opt$max_len
sv_table <- sv_table[, nchar(getSequences(sv_table)) %in% seq(min_len, max_len)]

print("Sequence lengths after length filtering:")
table(nchar(getSequences(sv_table)))

```


##Remove chimeras

DADA2 only considers "bimeras", or chimeras spawned from exactly two parents sequences.
```{r remove chimeras}

sv_table.no_chim <- removeBimeraDenovo(sv_table, method = "consensus", verbose = TRUE)

#check what percentage of reads remain
print("Percentage of reads remaining after bimera removal:")
sum(sv_table.no_chim) / sum(sv_table)

```


##Track read retention through the pipeline

See how many reads were retained or discarded at each stage of processing.
```{r track reads}

getN <- function(x) sum(getUniques(x))

if (length(sample_names) > 1){
  track_table <- cbind(filt_stats, sapply(dada_Fs, getN), sapply(mergers, getN), rowSums(sv_table), rowSums(sv_table.no_chim))
} else {
  track_table <- cbind(filt_stats, getN(dada_Fs), getN(mergers), sum(sv_table), sum(sv_table.no_chim))
}

colnames(track_table) <- c("raw", "filtered", "denoised", "merged", "tabled", "non_chim")
rownames(track_table) <- sample_names
print("Read counts at each stage of the DADA2 pipeline:")
track_table

save(mergers, sv_table, sv_table.no_chim, track_table, file = file.path(dada2_path, "tables.RData"))
write.table(sv_table.no_chim, file = file.path(dada2_path, "sv_table.no_chim.txt"), quote = FALSE, sep = "\t")

```

